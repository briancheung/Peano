{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of the paper \"Lateral Connections in Denoising Autoencoders Support Supervised Learning\"\n",
    "\n",
    "What's remarkable about [Rasmus et. al. 2015](http://arxiv.org/abs/1504.08215) is that they are able to achieve state-of-the-art performance on permutation invariant MNIST without dropout (though the denoising step could be performing a similar form of regularization). Unlike previous work with autoencoders, they acheive this great performance with the same semi-supervised cost function for the entire training process.\n",
    "\n",
    "It's also worth noting their model claims by far the best semisupervised performance with about ~.75% error with only 500 labeled MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import peano\n",
    "import peano.pops as P\n",
    "from pylearn2.space import CompositeSpace, VectorSpace\n",
    "\n",
    "dtype = theano.config.floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "\n",
    "The lateral connections in this model make it slightly more tedious to build since many layers depend on multiple previous layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z1 = P.nnet.Sequential('z1')\n",
    "z1.add(P.nnet.Linear(784, 1000))\n",
    "z1.add(P.nnet.BatchNormalization(1000))\n",
    "\n",
    "z2 = P.nnet.Sequential('z2')\n",
    "z2.add(P.nnet.Linear(1000, 500))\n",
    "z2.add(P.nnet.BatchNormalization(500))\n",
    "\n",
    "z3 = P.nnet.Sequential('z3')\n",
    "z3.add(P.nnet.Linear(500, 250))\n",
    "z3.add(P.nnet.BatchNormalization(250))\n",
    "\n",
    "z4 = P.nnet.Sequential('z4')\n",
    "z4.add(P.nnet.Linear(250, 250))\n",
    "z4.add(P.nnet.BatchNormalization(250))\n",
    "\n",
    "z5 = P.nnet.Sequential('z5')\n",
    "z5.add(P.nnet.Linear(250, 250))\n",
    "z5.add(P.nnet.BatchNormalization(250))\n",
    "\n",
    "ll0 = P.nnet.Lateral(784)\n",
    "ll1 = P.nnet.Lateral(1000)\n",
    "ll2 = P.nnet.Lateral(500)\n",
    "ll3 = P.nnet.Lateral(250)\n",
    "ll4 = P.nnet.Lateral(250)\n",
    "ll5 = P.nnet.Lateral(250)\n",
    "ll6 = P.nnet.Lateral(10)\n",
    "\n",
    "u6 = P.nnet.Linear(10, 250)\n",
    "u5 = P.nnet.Linear(250, 250)\n",
    "u4 = P.nnet.Linear(250, 250)\n",
    "u3 = P.nnet.Linear(250, 500)\n",
    "u2 = P.nnet.Linear(500, 1000)\n",
    "u1 = P.nnet.Linear(1000, 784)\n",
    "\n",
    "sl = P.nnet.Sequential('sl')\n",
    "sl.add(P.nnet.Linear(250, 10))\n",
    "sl.add(T.nnet.softmax)\n",
    "\n",
    "xt = T.matrix(dtype=dtype)\n",
    "\n",
    "z1f = z1.apply(xt)\n",
    "h1 = T.nnet.relu(z1f)\n",
    "\n",
    "z2f = z2.apply(h1)\n",
    "h2 = T.nnet.relu(z2f)\n",
    "\n",
    "z3f = z3.apply(h2)\n",
    "h3 = T.nnet.relu(z3f)\n",
    "\n",
    "z4f = z4.apply(h3)\n",
    "h4 = T.nnet.relu(z4f)\n",
    "\n",
    "z5f = z5.apply(h4)\n",
    "h5 = T.nnet.relu(z5f)\n",
    "\n",
    "y_s = sl.apply(z5f)\n",
    "zh6 = ll6.apply(y_s, 0.)\n",
    "u6f = u6.apply(zh6)\n",
    "\n",
    "zh5 = ll5.apply(z5f, u6f)\n",
    "u5f = u5.apply(zh5)\n",
    "\n",
    "zh4 = ll4.apply(z4f, u5f)\n",
    "u4f = u4.apply(zh4)\n",
    "\n",
    "zh3 = ll3.apply(z3f, u4f)\n",
    "u3f = u3.apply(zh3)\n",
    "\n",
    "zh2 = ll2.apply(z2f, u3f)\n",
    "u2f = u2.apply(zh2)\n",
    "\n",
    "zh1 = ll1.apply(z1f, u2f)\n",
    "u1f = u1.apply(zh1)\n",
    "\n",
    "xh = ll0.apply(xt, u1f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the parameters and construct the cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = []\n",
    "for l in [z1,z2,z3,z4,z5,ll0,ll1,ll2,ll3,ll4,ll5,ll6,u6,u5,u4,u3,u2,u1,sl]:\n",
    "    params += l.params\n",
    "\n",
    "x_true = T.matrix(dtype=dtype)\n",
    "y_true = T.matrix(dtype=dtype)\n",
    "lr = T.scalar(dtype=dtype)\n",
    "\n",
    "r_cost = P.cost.mean_squared_error(x_true, xh)\n",
    "s_cost = P.cost.cross_entropy(y_true, y_s)\n",
    "\n",
    "cost = s_cost + 500.*r_cost\n",
    "misclass_cost = T.neq(T.argmax(y_true, axis=1), T.argmax(y_s, axis=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take derivatives and compile the appropriate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.cmodule): WARNING: your Theano flags `gcc.cxxflags` specify an `-march=X` flags.\n",
      "         It is better to let Theano/g++ find it automatically, but we don't do it now\n",
      "WARNING:theano.gof.cmodule:WARNING: your Theano flags `gcc.cxxflags` specify an `-march=X` flags.\n",
      "         It is better to let Theano/g++ find it automatically, but we don't do it now\n"
     ]
    }
   ],
   "source": [
    "gparams = T.grad(cost, wrt=params)\n",
    "updates = peano.optimizer.adam_update(params, gparams, alpha=lr)\n",
    "\n",
    "learn_mlp_fn = theano.function(inputs = [xt, x_true, y_true, lr],\n",
    "                                outputs = cost,\n",
    "                                updates = updates)\n",
    "\n",
    "misclass_mlp_fn = theano.function(inputs = [xt, y_true],\n",
    "                                    outputs = misclass_cost)\n",
    "\n",
    "encode_mlp_fn = theano.function(inputs = [xt],\n",
    "                                    outputs = xh)\n",
    "\n",
    "decode_mlp_fn = theano.function(inputs = [xt, y_s],\n",
    "                                    outputs = xh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordance with the paper, we are training on the entire MNIST training set (all 60000 digits). After 100 epochs we evalute on the MNIST test set (10000 digits). Since this is the actual test set, we are not allowed to tweak anything. The test set error is the final error for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 14.3241860867 seconds\n",
      "epoch 1 14.3138051033 seconds\n",
      "epoch 2 14.302243948 seconds\n",
      "epoch 3 14.2785608768 seconds\n",
      "epoch 4 14.2825241089 seconds\n",
      "epoch 5 14.2982139587 seconds\n",
      "epoch 6 14.3066959381 seconds\n",
      "epoch 7 14.3182621002 seconds\n",
      "epoch 8 14.2831978798 seconds\n",
      "epoch 9 14.3160161972 seconds\n",
      "epoch 10 14.3053920269 seconds\n",
      "epoch 11 14.277520895 seconds\n",
      "epoch 12 14.2811539173 seconds\n",
      "epoch 13 14.2720370293 seconds\n",
      "epoch 14 14.30133605 seconds\n",
      "epoch 15 14.285476923 seconds\n",
      "epoch 16 14.2630209923 seconds\n",
      "epoch 17 14.2744040489 seconds\n",
      "epoch 18 14.2751760483 seconds\n",
      "epoch 19 14.2680740356 seconds\n",
      "epoch 20 14.2793338299 seconds\n",
      "epoch 21 14.2843091488 seconds\n",
      "epoch 22 14.2869808674 seconds\n",
      "epoch 23 14.276250124 seconds\n",
      "epoch 24 14.2990670204 seconds\n",
      "epoch 25 14.2792639732 seconds\n",
      "epoch 26 14.2720110416 seconds\n",
      "epoch 27 14.2625980377 seconds\n",
      "epoch 28 14.259770155 seconds\n",
      "epoch 29 14.2725348473 seconds\n",
      "epoch 30 14.3095350266 seconds\n",
      "epoch 31 14.3106970787 seconds\n",
      "epoch 32 14.3134691715 seconds\n",
      "epoch 33 14.3098089695 seconds\n",
      "epoch 34 14.2686629295 seconds\n",
      "epoch 35 14.267441988 seconds\n",
      "epoch 36 14.2607939243 seconds\n",
      "epoch 37 14.2618298531 seconds\n",
      "epoch 38 14.2534959316 seconds\n",
      "epoch 39 14.2622108459 seconds\n",
      "epoch 40 14.2575678825 seconds\n",
      "epoch 41 14.2629668713 seconds\n",
      "epoch 42 14.2600951195 seconds\n",
      "epoch 43 14.2706940174 seconds\n",
      "epoch 44 14.2730967999 seconds\n",
      "epoch 45 14.2634661198 seconds\n",
      "epoch 46 14.2567288876 seconds\n",
      "epoch 47 14.2662849426 seconds\n",
      "epoch 48 14.2648868561 seconds\n",
      "epoch 49 14.2646889687 seconds\n",
      "epoch 50 14.2747149467 seconds\n",
      "epoch 51 14.284815073 seconds\n",
      "epoch 52 14.3004801273 seconds\n",
      "epoch 53 14.2752230167 seconds\n",
      "epoch 54 14.3085579872 seconds\n",
      "epoch 55 14.2957808971 seconds\n",
      "epoch 56 14.3062949181 seconds\n",
      "epoch 57 14.2829530239 seconds\n",
      "epoch 58 14.2828259468 seconds\n",
      "epoch 59 14.3139169216 seconds\n",
      "epoch 60 14.2948460579 seconds\n",
      "epoch 61 14.2714400291 seconds\n",
      "epoch 62 14.2658820152 seconds\n",
      "epoch 63 14.258245945 seconds\n",
      "epoch 64 14.295976162 seconds\n",
      "epoch 65 14.2694180012 seconds\n",
      "epoch 66 14.2725749016 seconds\n",
      "epoch 67 14.3092057705 seconds\n",
      "epoch 68 14.2872700691 seconds\n",
      "epoch 69 14.2699389458 seconds\n",
      "epoch 70 14.2696049213 seconds\n",
      "epoch 71 14.2747981548 seconds\n",
      "epoch 72 14.2779450417 seconds\n",
      "epoch 73 14.2599079609 seconds\n",
      "epoch 74 14.2562818527 seconds\n",
      "epoch 75 14.2988798618 seconds\n",
      "epoch 76 14.2895288467 seconds\n",
      "epoch 77 14.258767128 seconds\n",
      "epoch 78 14.2590417862 seconds\n",
      "epoch 79 14.265556097 seconds\n",
      "epoch 80 14.289244175 seconds\n",
      "epoch 81 14.2696130276 seconds\n",
      "epoch 82 14.2715451717 seconds\n",
      "epoch 83 14.268184185 seconds\n",
      "epoch 84 14.2826249599 seconds\n",
      "epoch 85 14.2723329067 seconds\n",
      "epoch 86 14.2629330158 seconds\n",
      "epoch 87 14.2630178928 seconds\n",
      "epoch 88 14.2938277721 seconds\n",
      "epoch 89 14.3064641953 seconds\n",
      "epoch 90 14.2896800041 seconds\n",
      "epoch 91 14.2934010029 seconds\n",
      "epoch 92 14.3221580982 seconds\n",
      "epoch 93 14.3061950207 seconds\n",
      "epoch 94 14.311524868 seconds\n",
      "epoch 95 14.3170070648 seconds\n",
      "epoch 96 14.2952461243 seconds\n",
      "epoch 97 14.2871549129 seconds\n",
      "epoch 98 14.3108949661 seconds\n",
      "epoch 99 14.3015282154 seconds\n",
      "Test set error: 0.0092\n"
     ]
    }
   ],
   "source": [
    "from pylearn2.datasets import mnist\n",
    "ds = mnist.MNIST(which_set = 'train', start=0, stop=60000)\n",
    "val = mnist.MNIST(which_set = 'test', start=0, stop=10000)\n",
    "val_X, val_y = val.get_data()\n",
    "val_y = np.squeeze(np.eye(10)[val_y]).astype(dtype)\n",
    "\n",
    "data_space = VectorSpace(dim=784)\n",
    "label_space = VectorSpace(dim= 10)\n",
    "\n",
    "lrd = np.linspace(.002,0.,50).astype(dtype)\n",
    "for i in range(100):\n",
    "    cost = 0.\n",
    "    misclass = 0.\n",
    "    ds_iter = ds.iterator(mode='sequential', batch_size=100, data_specs=(CompositeSpace((data_space, label_space)), ('features', 'targets')))\n",
    "    t0 = time.time()\n",
    "    for X,y in ds_iter:\n",
    "        if i < 50:\n",
    "            learn_mlp_fn(X+0.3*np.random.randn(*X.shape).astype(dtype) , X, y, 0.002)\n",
    "        else:\n",
    "            learn_mlp_fn(X+0.3*np.random.randn(*X.shape).astype(dtype) , X, y, lrd[i-50])\n",
    "    print 'epoch', i, time.time()-t0, 'seconds'\n",
    "print 'Test set error:', misclass_mlp_fn(val_X, val_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with everything said and done, the performance we acheive is ~.92% error on the test set which is very good but off from the .68% claimed in the paper. There could be finicky parameters like weight initilization that might account for the difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
