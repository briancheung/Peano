{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of the paper \"Lateral Connections in Denoising Autoencoders Support Supervised Learning\"\n",
    "\n",
    "What's remarkable about [Rasmus et. al. 2015](http://arxiv.org/abs/1504.08215) is that they are able to achieve state-of-the-art performance on permutation invariant MNIST without dropout (though the denoising step could be performing a similar form of regularization). Unlike previous work with autoencoders, they acheive this great performance with the same semi-supervised cost function for the entire training process.\n",
    "\n",
    "It's also worth noting their model claims by far the best semisupervised performance with about ~.75% error with only 500 labeled MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import peano\n",
    "import peano.pops as P\n",
    "from pylearn2.space import CompositeSpace, VectorSpace\n",
    "\n",
    "dtype = theano.config.floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "\n",
    "The lateral connections in this model make it slightly more tedious to build since many layers depend on multiple previous layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z1 = P.nnet.Sequential('z1')\n",
    "z1.add(P.nnet.Linear(784, 1000))\n",
    "z1.add(P.nnet.BatchNormalization(1000))\n",
    "\n",
    "z2 = P.nnet.Sequential('z2')\n",
    "z2.add(P.nnet.Linear(1000, 500))\n",
    "z2.add(P.nnet.BatchNormalization(500))\n",
    "\n",
    "z3 = P.nnet.Sequential('z3')\n",
    "z3.add(P.nnet.Linear(500, 250))\n",
    "z3.add(P.nnet.BatchNormalization(250))\n",
    "\n",
    "z4 = P.nnet.Sequential('z4')\n",
    "z4.add(P.nnet.Linear(250, 250))\n",
    "z4.add(P.nnet.BatchNormalization(250))\n",
    "\n",
    "z5 = P.nnet.Sequential('z5')\n",
    "z5.add(P.nnet.Linear(250, 250))\n",
    "z5.add(P.nnet.BatchNormalization(250))\n",
    "\n",
    "ll0 = P.nnet.Lateral(784)\n",
    "ll1 = P.nnet.Lateral(1000)\n",
    "ll2 = P.nnet.Lateral(500)\n",
    "ll3 = P.nnet.Lateral(250)\n",
    "ll4 = P.nnet.Lateral(250)\n",
    "ll5 = P.nnet.Lateral(250)\n",
    "ll6 = P.nnet.Lateral(10)\n",
    "\n",
    "u6 = P.nnet.Linear(10, 250)\n",
    "u5 = P.nnet.Linear(250, 250)\n",
    "u4 = P.nnet.Linear(250, 250)\n",
    "u3 = P.nnet.Linear(250, 500)\n",
    "u2 = P.nnet.Linear(500, 1000)\n",
    "u1 = P.nnet.Linear(1000, 784)\n",
    "\n",
    "sl = P.nnet.Sequential('sl')\n",
    "sl.add(P.nnet.Linear(250, 10))\n",
    "sl.add(T.nnet.softmax)\n",
    "\n",
    "xt = T.matrix(dtype=dtype)\n",
    "\n",
    "z1f = z1.apply(xt)\n",
    "h1 = T.nnet.relu(z1f)\n",
    "\n",
    "z2f = z2.apply(h1)\n",
    "h2 = T.nnet.relu(z2f)\n",
    "\n",
    "z3f = z3.apply(h2)\n",
    "h3 = T.nnet.relu(z3f)\n",
    "\n",
    "z4f = z4.apply(h3)\n",
    "h4 = T.nnet.relu(z4f)\n",
    "\n",
    "z5f = z5.apply(h4)\n",
    "h5 = T.nnet.relu(z5f)\n",
    "\n",
    "y_s = sl.apply(z5f)\n",
    "zh6 = ll6.apply(y_s, 0.)\n",
    "u6f = u6.apply(zh6)\n",
    "\n",
    "zh5 = ll5.apply(z5f, u6f)\n",
    "u5f = u5.apply(zh5)\n",
    "\n",
    "zh4 = ll4.apply(z4f, u5f)\n",
    "u4f = u4.apply(zh4)\n",
    "\n",
    "zh3 = ll3.apply(z3f, u4f)\n",
    "u3f = u3.apply(zh3)\n",
    "\n",
    "zh2 = ll2.apply(z2f, u3f)\n",
    "u2f = u2.apply(zh2)\n",
    "\n",
    "zh1 = ll1.apply(z1f, u2f)\n",
    "u1f = u1.apply(zh1)\n",
    "\n",
    "xh = ll0.apply(xt, u1f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the parameters and construct the cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = []\n",
    "for l in [z1,z2,z3,z4,z5,ll0,ll1,ll2,ll3,ll4,ll5,ll6,u6,u5,u4,u3,u2,u1,sl]:\n",
    "    params += l.params\n",
    "\n",
    "x_true = T.matrix(dtype=dtype)\n",
    "y_true = T.matrix(dtype=dtype)\n",
    "lr = T.scalar(dtype=dtype)\n",
    "\n",
    "r_cost = P.cost.mean_squared_error(x_true, xh)\n",
    "s_cost = P.cost.cross_entropy(y_true, y_s)\n",
    "\n",
    "cost = s_cost + 500.*r_cost\n",
    "misclass_cost = T.neq(T.argmax(y_true, axis=1), T.argmax(y_s, axis=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take derivatives and compile the appropriate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.cmodule): WARNING: your Theano flags `gcc.cxxflags` specify an `-march=X` flags.\n",
      "         It is better to let Theano/g++ find it automatically, but we don't do it now\n",
      "WARNING:theano.gof.cmodule:WARNING: your Theano flags `gcc.cxxflags` specify an `-march=X` flags.\n",
      "         It is better to let Theano/g++ find it automatically, but we don't do it now\n"
     ]
    }
   ],
   "source": [
    "gparams = T.grad(cost, wrt=params)\n",
    "updates = peano.optimizer.adam_update(params, gparams, alpha=lr)\n",
    "\n",
    "learn_mlp_fn = theano.function(inputs = [xt, x_true, y_true, lr],\n",
    "                                outputs = cost,\n",
    "                                updates = updates)\n",
    "\n",
    "misclass_mlp_fn = theano.function(inputs = [xt, y_true],\n",
    "                                    outputs = misclass_cost)\n",
    "\n",
    "encode_mlp_fn = theano.function(inputs = [xt],\n",
    "                                    outputs = xh)\n",
    "\n",
    "decode_mlp_fn = theano.function(inputs = [xt, y_s],\n",
    "                                    outputs = xh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordance with the paper, we are training on the entire MNIST training set (all 60000 digits). After 100 epochs we evalute on the MNIST test set (10000 digits). Since this is the actual test set, we are not allowed to tweak anything. The test set error is the final error for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 13.8872830868 seconds\n",
      "epoch 1 13.8682770729 seconds\n",
      "epoch 2 13.8656430244 seconds\n",
      "epoch 3 13.865281105 seconds\n",
      "epoch 4 13.8579039574 seconds\n",
      "epoch 5 13.8575110435 seconds\n",
      "epoch 6 13.8589138985 seconds\n",
      "epoch 7 13.8591980934 seconds\n",
      "epoch 8 13.8824970722 seconds\n",
      "epoch 9 13.8555669785 seconds\n",
      "epoch 10 13.8619389534 seconds\n",
      "epoch 11 13.810571909 seconds\n",
      "epoch 12 13.8109958172 seconds\n",
      "epoch 13 13.8124639988 seconds\n",
      "epoch 14 13.8127009869 seconds\n",
      "epoch 15 13.8124508858 seconds\n",
      "epoch 16 13.810546875 seconds\n",
      "epoch 17 13.809376955 seconds\n",
      "epoch 18 13.8086268902 seconds\n",
      "epoch 19 13.8161821365 seconds\n",
      "epoch 20 13.8320550919 seconds\n",
      "epoch 21 13.8239459991 seconds\n",
      "epoch 22 13.8049168587 seconds\n",
      "epoch 23 13.8075540066 seconds\n",
      "epoch 24 13.8086600304 seconds\n",
      "epoch 25 13.8068599701 seconds\n",
      "epoch 26 13.8096899986 seconds\n",
      "epoch 27 13.8059720993 seconds\n",
      "epoch 28 13.812474966 seconds\n",
      "epoch 29 13.8103308678 seconds\n",
      "epoch 30 13.8111639023 seconds\n",
      "epoch 31 13.8059959412 seconds\n",
      "epoch 32 13.8834118843 seconds\n",
      "epoch 33 13.8549790382 seconds\n",
      "epoch 34 13.8500959873 seconds\n",
      "epoch 35 13.8462190628 seconds\n",
      "epoch 36 13.8492290974 seconds\n",
      "epoch 37 13.8467550278 seconds\n",
      "epoch 38 13.849159956 seconds\n",
      "epoch 39 13.8481991291 seconds\n",
      "epoch 40 13.8483719826 seconds\n",
      "epoch 41 13.8599989414 seconds\n",
      "epoch 42 13.8579909801 seconds\n",
      "epoch 43 13.8541522026 seconds\n",
      "epoch 44 13.8569998741 seconds\n",
      "epoch 45 13.8742978573 seconds\n",
      "epoch 46 13.8649029732 seconds\n",
      "epoch 47 13.8641240597 seconds\n",
      "epoch 48 13.8665149212 seconds\n",
      "epoch 49 13.8670017719 seconds\n",
      "epoch 50 13.8571679592 seconds\n",
      "epoch 51 13.8592450619 seconds\n",
      "epoch 52 13.860779047 seconds\n",
      "epoch 53 13.8654670715 seconds\n",
      "epoch 54 13.8769090176 seconds\n",
      "epoch 55 13.8597249985 seconds\n",
      "epoch 56 13.8486568928 seconds\n",
      "epoch 57 13.8706259727 seconds\n",
      "epoch 58 13.8613469601 seconds\n",
      "epoch 59 13.8541488647 seconds\n",
      "epoch 60 13.860637188 seconds\n",
      "epoch 61 13.872797966 seconds\n",
      "epoch 62 13.8611280918 seconds\n",
      "epoch 63 13.8574798107 seconds\n",
      "epoch 64 13.8553860188 seconds\n",
      "epoch 65 13.8548741341 seconds\n",
      "epoch 66 13.8535559177 seconds\n",
      "epoch 67 13.8536939621 seconds\n",
      "epoch 68 13.8461520672 seconds\n",
      "epoch 69 13.8502149582 seconds\n",
      "epoch 70 13.8633480072 seconds\n",
      "epoch 71 13.8466999531 seconds\n",
      "epoch 72 13.8481240273 seconds\n",
      "epoch 73 13.8589370251 seconds\n",
      "epoch 74 13.8615231514 seconds\n",
      "epoch 75 13.8705339432 seconds\n",
      "epoch 76 13.8678529263 seconds\n",
      "epoch 77 13.8646609783 seconds\n",
      "epoch 78 13.8684680462 seconds\n",
      "epoch 79 13.8590300083 seconds\n",
      "epoch 80 13.9011671543 seconds\n",
      "epoch 81 13.8153989315 seconds\n",
      "epoch 82 13.8281509876 seconds\n",
      "epoch 83 13.8092107773 seconds\n",
      "epoch 84 13.808519125 seconds\n",
      "epoch 85 13.8095328808 seconds\n",
      "epoch 86 13.8204059601 seconds\n",
      "epoch 87 13.8251078129 seconds\n",
      "epoch 88 13.8207719326 seconds\n",
      "epoch 89 13.8242650032 seconds\n",
      "epoch 90 13.8284039497 seconds\n",
      "epoch 91 13.830078125 seconds\n",
      "epoch 92 13.8238298893 seconds\n",
      "epoch 93 13.837916851 seconds\n",
      "epoch 94 13.8314900398 seconds\n",
      "epoch 95 13.8597671986 seconds\n",
      "epoch 96 13.8281450272 seconds\n",
      "epoch 97 13.8222389221 seconds\n",
      "epoch 98 13.8630461693 seconds\n",
      "epoch 99 13.8262369633 seconds\n",
      "Test set error: 0.0091\n"
     ]
    }
   ],
   "source": [
    "from pylearn2.datasets import mnist\n",
    "ds = mnist.MNIST(which_set = 'train', start=0, stop=60000)\n",
    "val = mnist.MNIST(which_set = 'test', start=0, stop=10000)\n",
    "val_X, val_y = val.get_data()\n",
    "val_y = np.squeeze(np.eye(10)[val_y]).astype(dtype)\n",
    "\n",
    "data_space = VectorSpace(dim=784)\n",
    "label_space = VectorSpace(dim= 10)\n",
    "\n",
    "lrd = np.linspace(.002,0.,50).astype(dtype)\n",
    "for i in range(100):\n",
    "    cost = 0.\n",
    "    misclass = 0.\n",
    "    ds_iter = ds.iterator(mode='sequential', batch_size=100, data_specs=(CompositeSpace((data_space, label_space)), ('features', 'targets')))\n",
    "    t0 = time.time()\n",
    "    for X,y in ds_iter:\n",
    "        if i < 50:\n",
    "            learn_mlp_fn(X+0.3*np.random.randn(*X.shape).astype(dtype) , X, y, 0.002)\n",
    "        else:\n",
    "            learn_mlp_fn(X+0.3*np.random.randn(*X.shape).astype(dtype) , X, y, lrd[i-50])\n",
    "    print 'epoch', i, time.time()-t0, 'seconds'\n",
    "print 'Test set error:', misclass_mlp_fn(val_X, val_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with everything said and done, the performance we acheive is .91% error on the test set which is very good but off from the .68% claimed in the paper. There could be finicky parameters like weight initilization that might account for the difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
